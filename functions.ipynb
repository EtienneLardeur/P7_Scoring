{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# prepare data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# models and metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "# \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_show_results(model, data, split_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = my_split(data=data, split_size=split_size)\n",
    "    model.fit(X_train, y_train)\n",
    "    results = model.predict_proba(X_test)[:, 1]\n",
    "    print('Auc Score: %f' %roc_auc_score(y_test, results))\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.title(str(model) + 'Roc curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, X_train, X_test, y_train, y_test, case='case'):\n",
    "    model.fit(X_train, y_train)\n",
    "    results = model.predict_proba(X_test)[:, 1]\n",
    "    plt.title('Failure probabilities for '  + case)\n",
    "    plt.xlabel('Probabilities')\n",
    "    plt.ylabel('Count')\n",
    "    plt.hist(results, bins=100)\n",
    "    plt.show()\n",
    "    # print('Auc Score: %f' %roc_auc_score(y_test, results))\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.title('Roc curve of ' + case)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_plot(df, feature):\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    # KDE plot of loans that were repaid on time\n",
    "    sns.kdeplot(df.loc[df['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # KDE plot of loans which were not repaid on time\n",
    "    sns.kdeplot(df.loc[df['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    # Labeling of plot\n",
    "    plt.xlabel(str(feature))\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of ' + str(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_corrs(df):\n",
    "    corrs = []\n",
    "    for col in df.columns: \n",
    "        if col != 'TARGET':\n",
    "            corr = df['TARGET'].corr(df[col], method='pearson')\n",
    "            corrs.append((corr))\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('TARGET')\n",
    "    df_corrs = pd.DataFrame()\n",
    "    df_corrs['feature'] = cols\n",
    "    df_corrs['abs_corr'] = np.abs(corrs)\n",
    "    df_corrs = df_corrs.sort_values(by=['abs_corr'], ascending=False)\n",
    "    return df_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df):\n",
    "    le_count = 0\n",
    "    le_column = []\n",
    "    le = LabelEncoder()\n",
    "    for col in df:\n",
    "        if df[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            if len(list(df[col].unique())) <= 2:\n",
    "                # Train on the training data\n",
    "                le.fit(df[col])\n",
    "                # Transform both training and testing data\n",
    "                df[col] = le.transform(df[col])\n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "                le_column.append(col)\n",
    "                      \n",
    "    print('%d columns were label encoded.' % le_count , le_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric(df, group_var, df_name):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string): \n",
    "            the variable by which to group df\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            the statistics (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the perfect heatmap\n",
    "\n",
    "def heatmap(x, y, size, color):\n",
    "    \"\"\"\n",
    "    built out of a tutorial seen on towardsdatascience.com\n",
    "\n",
    "    Enhance heatmap for feature correlation observation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    # Mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in x.unique()]\n",
    "    y_labels = [v for v in y.unique()]\n",
    "    x_to_num = {p[1]: p[0] for p in enumerate(x_labels)}\n",
    "    y_to_num = {p[1]: p[0] for p in enumerate(y_labels)}\n",
    "    size_scale = 500\n",
    "    # Use 256 colors for the diverging color palette\n",
    "    n_colors = 256\n",
    "    # Create the palette\n",
    "    palette = sns.diverging_palette(20, 220, n=n_colors)\n",
    "    # Range of values mapped to the palt, i.e. min and max poss corr\n",
    "    color_min, color_max = [-1, 1]\n",
    "\n",
    "    def value_to_color(val):\n",
    "        # pos of value in input range, relative to length of input range\n",
    "        val_position = float((val - color_min)) / (color_max - color_min)\n",
    "        # target index in the color palette\n",
    "        ind = int(val_position * (n_colors - 1))\n",
    "        return palette[ind]\n",
    "\n",
    "    # Setup a 1x40 Grid\n",
    "    plot_grid = plt.GridSpec(1, 40, hspace=0.2, wspace=0.1)\n",
    "    # Use the leftmost 39 columns of the grid for the main plot\n",
    "    ax = plt.subplot(plot_grid[:, :-1])\n",
    "\n",
    "    ax.scatter(\n",
    "        x=x.map(x_to_num),  # Use mapping for x\n",
    "        y=y.map(y_to_num),  # Use mapping for y\n",
    "        s=size * size_scale,  # Vector sq sizes\n",
    "        c=color.apply(value_to_color),  # Vector sq color values\n",
    "        marker='s'  # Use square as scatterplot marker\n",
    "    )\n",
    "\n",
    "    # Show column labels on the axes\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.grid(False, 'major')\n",
    "    ax.grid(True, 'minor')\n",
    "    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n",
    "    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n",
    "\n",
    "    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n",
    "    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n",
    "\n",
    "    # Add color legend on the right side of the plot\n",
    "    ax = plt.subplot(plot_grid[:, -1])  # Use the rightmost column of the plot\n",
    "\n",
    "    # Fixed x coordinate for the bars\n",
    "    col_x = [0]*len(palette)\n",
    "    # y coordinates for each of the n_colors bars\n",
    "    bar_y = np.linspace(color_min, color_max, n_colors)\n",
    "\n",
    "    bar_height = bar_y[1] - bar_y[0]\n",
    "    ax.barh(\n",
    "        y=bar_y,\n",
    "        width=[5]*len(palette),  # Make bars 5 units wide\n",
    "        left=col_x,  # Make bars start at 0\n",
    "        height=bar_height,\n",
    "        color=palette,\n",
    "        linewidth=0\n",
    "    )\n",
    "    # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n",
    "    ax.set_xlim(1, 2)\n",
    "    # Hide grid\n",
    "    ax.grid(False)\n",
    "    # Make background white\n",
    "    ax.set_facecolor('white')\n",
    "    # Remove horizontal ticks\n",
    "    ax.set_xticks([])\n",
    "    # Show vertical ticks for min, middle and max\n",
    "    ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3))\n",
    "    # Show vertical ticks on the right\n",
    "    ax.yaxis.tick_right()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_categorical(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    return categorical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
